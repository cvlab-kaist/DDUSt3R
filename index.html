<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DDUSt3R">
  <meta property="og:title" content="DDUSt3R"/>
  <meta property="og:description" content="Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="DDUSt3R">
  <meta name="twitter:description" content="DDUSt3R">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DDUSt3R</title>
  <link rel="icon" type="image/x-icon" href="assets/bulb.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="colorful_text">D<sup>2</sup>USt3R</span>: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://onground-korea.github.io" target="_blank">Jisang Han</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://hg010303.github.io/" target="_blank">Honggyu An</a><sup>1*</sup>,</span>
                  <span class="author-block">
                  <a href="https://crepejung00.github.io/" target="_blank">Jaewoo Jung</a><sup>1*</sup>,</span>
                  <span class="author-block">
                  Takuya Narihira<sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://j0seo.github.io/" target="_blank">Junyoung Seo</a><sup>1</sup>,</span>
                  <br>
                  <span class="author-block">
                    Kazumi Fukuda<sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://kchyun.github.io/" target="_blank">Chaehyun Kim</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://sunghwanhong.github.io/" target="_blank">Sunghwan Hong</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.yukimitsufuji.com/" target="_blank">Yuki Mitsufuji</a><sup>3,4‚Ä†</sup>,</span>
                  <span class="author-block">
                    <a href="https://cvlab.kaist.ac.kr/members/faculty" target="_blank">Seungryong Kim</a><sup>1‚Ä†</sup>
                  </span>
            </div>

                  <div class="is-size-5 publication-authors">
                      <span class="author-block"><sup>1</sup>KAIST AI, <sup>2</sup>Korea University, <sup>3</sup>Sony AI, <sup>4</sup>Sony Group Corporation</span>
                      <br>
                      <span class="author-block"><sup>*</sup>Co-first authors, <sup>‚Ä†</sup>Co-corresponding authors</span>
                      <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    </div>
                    <div>
                    <span class="is-size-5 publication-venue">ArXiv 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <svg class="svg-inline--fa fa-file-pdf fa-w-12" style="width: 24px; height: 24px;" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" style="width: 24px; height: 24px;" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span>
                    </div>
                </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image-container">
        <!-- <div class="zoomcaption">üîç Click to zoom in</div> -->
        <img class="teaser-image" src="assets/1_teaser.png">
        <figcaption>
          Given a pair of input views, our <b>D<sup>2</sup>USt3R</b> accurately establishes dense correspondence not only in static regions but also 
          in dynamic regions, enabling full reconstruction of a dynamic scene via our proposed 4D pointmap. As highlighted by 
          <span style="color:red;">red dots &#8226;</span> in the pointmap, DUSt3R and MonST3R align pointmaps solely based on camera motion, causing corresponding 
          2D pixels to become <i>misaligned</i> in 3D space. We compare the cross-attention maps, established correspondence fields, 
          and estimated depth maps produced by our <b>D<sup>2</sup>USt3R</b> against baseline methods.
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the task of 3D reconstruction in dynamic scenes, where object motions degrade the quality of 3D pointmap regression methods, such as DUSt3R, originally designed for static 3D scene reconstruction. Although these methods provide an elegant and powerful solution in static settings, they struggle in the presence of dynamic motions that disrupt alignment based solely on camera poses. To overcome this, we propose D<sup>2</sup>USt3R that regresses 4D pointmaps that simultaneiously capture both static and
            dynamic 3D scene geometry in a feed-forward manner. By explicitly incorporating both spatial and temporal aspects, our approach successfully encapsulates spatio-temporal dense correspondence to the proposed 4D poitnmaps, enhancing  performance of downstream tasks. Extensive experimental evaluations demonstrate that our proposed approach consistently achieves superior reconstruction performance across various datasets featuring complex motions. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Motivation</h2>
          <img class="teaser-image" src="assets/2_dust3r_attn.png">
          <h2 class="content has-text-justified">
            <strong>Cross-attention visualization of DUSt3R on static and dynamic scenes.</strong>
            We visualize the attention maps of the source image corresponding to the query point marked in red on the target image, both layer-wise and averaged across all layers. 
            While DUSt3R reliably captures geometric correspondences during 3D reconstruction, it fails to establish correspondences in dynamic regions.
            This limitation arises because its training signal assumes that both frames are static and can be modeled using rigid camera motion, which does not account for dynamic motion.
          </h2>
          <img class="teaser-image" src="assets/3_dynamic_attn.png">
          <h2 class="content has-text-justified">
            <strong>Comparison of cross-attention on dynamic scene.</strong>
            We visualize the attention maps of the source and target images from a dynamic scene in the above figure, both layer-wise and averaged across all layers. 
            Although MonST3R is trained on dynamic video, its training signal remains identical to that of DUSt3R. 
            As a result, it fails to achieve reliable alignment between frames, ultimately limiting its 3D reconstruction performance. 
            In contrast, our D<sup>2</sup>USt3R successfully establishes correspondences between dynamic frames, and thus has a stronger ability to estimate 3D shapes from dynamic motions
          </h2>
        </div>
      </div>
</div>
</section>

<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Construction of our alignment loss</h2>
            <img class="teaser-image" src="assets/4_alignment_loss.png">
            <h2 class="content has-text-justified">
              We propose a pipeline for generating a 4D pointmap while explicitly addressing occlusions caused by dynamic regions. 
              (a) From the input images, we obtain optical flow refined via cycle consistency checks and derive a dynamic mask M<sub>dyn</sub>. 
              To align image I<sup>2</sup> with image I<sup>1</sup>, we utilize: 1) the camera pose to align static regions in I<sup>2</sup>, and 2) optical flow to align dynamic regions. 
              The alignment process is conducted specifically within regions corresponding to the <span style="color: red;">c</span><span style="color: orange;">o</span><span style="color: yellow;">l</span><span style="color: green;">o</span><span style="color: blue;">r</span><span style="color: purple;">e</span><span style="color: violet;">d</span> pixels. 
              Through this process, we construct a 4D pointmap, enabling alignment in 3D space for all corresponding 2D pixels.
            </h2>
          </div>
        </div>
</div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Qualitative results</h2>
      <h3 class="title is-4 has-text-centered">Pointmap reconstruction</h3>
      <img src="assets/5_pointmap_qual.png" alt="Teaser image" class="teaser-image">
      <!-- <h2 class="content has-text-justified"> -->
        <!-- <div style="text-align: center;"><strong>Qualitative results for pointmap reconstruction.</strong></div> -->
        <!-- We qualitatively compare the 3D pointmap of D<sup>2</sup>USt3R against other pointmap regression models. -->
        <!-- It is notable that both DUSt3R and MonST3R struggle to accurately reconstruct scenes that include dynamic elements.  -->
        <!-- We find that inaccurately established correspondence fields between dynamic regions negatively affect the overall reconstruction performance. -->
      <!-- </h2> -->
      <br><br>
      <h3 class="title is-4 has-text-centered">Depth estimation</h3>
      <img src="assets/6_depth_qual.png" alt="Teaser image" class="teaser-image">
      <!-- <div style="text-align: center;"><strong>epth estimation.</strong></div> -->
      <br> <br>
      <h3 class="title is-4 has-text-centered">Correspondences</h3>
      <img src="assets/7_matching_qual.png" alt="Teaser image" class="teaser-image">
      <br> <br>
      <h3 class="title is-4 has-text-centered">Optical flow estimation</h3>
      <img src="assets/8_flow_qual.png" alt="Teaser image" class="teaser-image">
      <!-- <div style="text-align: center;"><strong>Visualization of correspondences given a pair of images.</strong></div> -->
        
    </div>
    </div>
  </div>
</section>



<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
        <h2 class="content has-text-justified">
        </h2>
        We evaluate our method on depth estimation and camera pose estimation. 
        We additionally evaluate pointmap alignment accuracy in dynamic regions. 
        We compare our method against existing state-of-the-art pointmap regression models, specifically DUSt3R, MASt3R, and MonST3R.
        
        *: Reproduced with same dataset as Ours.

        <br><br>
        
        <h3 class="title is-4 has-text-centered">Multi-frame depth estimation results</h3>
        <img src="assets/multi_depth_quan.png" alt="Method image" class="method">
        <h2 class="content has-text-justified">
          <!-- <strong>Multi-frame depth estimation results.</strong>
          We compare multi-frame depth for both the entire scene and dynamic regions separately. 
          The comparison for dynamic regions is conducted only when the dynamic parts are identifiable. 
          *: Reproduced with same dataset as Ours. -->
        </h2>

        <h3 class="title is-4 has-text-centered">Single-frame depth estimation results</h3>
        <img src="assets/single_depth_quan.png" alt="Method image" class="method" style="transform: scale(0.7);">
        <h2 class="content has-text-justified">
          <!-- <strong>Single-frame depth estimation results.</strong>
          *: Reproduced with same dataset as Ours. -->
        </h2>

        <h3 class="title is-4 has-text-centered">Camera pose estimation results</h3>
        <img src="assets/pose_quan.png" alt="Method image" class="method">
        <h2 class="content has-text-justified">
          <!-- <strong>Camera pose estimation results.</strong>
          *: Reproduced with same dataset as Ours. -->
        </h2>
        <h3 class="title is-4 has-text-centered">Evaluation of pointmap alignment accuracy in dynamic objects</h3>
        <div style="text-align: center;">
          <img src="assets/alignment_quan.png" alt="Method image" class="method" width="40%">
        </div>
      </div>
    </div>
    <div class="hero-body">

    </div>
  </div>
</section>




<!-- BibTex citation -->
 <!-- <section class="section hero" id="BibTeX">
   <div class="container is-max-desktop content">
     <h2 class="title">BibTeX</h2>
     <pre><code>
     @misc{han2025d2ust3r,
     title={D<sup>2</sup>USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes},
     author={Jisang Han and Honggyu An and Jaewoo Jung and Takuya Narihira and Junyoung Seo and Kazumi Fukuda and Chaehyun Kim and Sunghwan Hong and Yuki Mitsufuji and Seungryong Kim},
     year={2025},
     eprint={2412.01471},
     archivePrefix={arXiv},
     primaryClass={cs.CV},
     url={https://arxiv.org/abs/2412.01471},}
    </code></pre>
   </div>
</section> -->
<!-- End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<script>
  document.addEventListener("DOMContentLoaded", () => {
    const images = document.querySelectorAll(".teaser-image");
    const zoomOverlay = document.createElement("div");
    zoomOverlay.classList.add("zoom-overlay");

    const zoomImage = document.createElement("img");
    zoomOverlay.appendChild(zoomImage);
    document.body.appendChild(zoomOverlay);

    images.forEach((img) => {
      // Preload high-res image if available
      const highResSrc = img.getAttribute("data-highres");
      if (highResSrc) {
        const preloadImg = new Image();
        preloadImg.src = highResSrc;
      }

      img.addEventListener("click", () => {
        zoomImage.src = highResSrc || img.src;
        zoomOverlay.classList.add("active");
      });
    });

    // Close the zoom overlay when clicking anywhere on it.
    zoomOverlay.addEventListener("click", () => {
      zoomOverlay.classList.remove("active");
      zoomImage.src = "";
    });

    // Allow closing with the ESC key.
    document.addEventListener("keydown", (e) => {
      if (e.key === "Escape") zoomOverlay.classList.remove("active");
    });
  });
</script>


  </body>
  </html>
